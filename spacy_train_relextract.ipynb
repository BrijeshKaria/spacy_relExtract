{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import re \n",
    "import string \n",
    "import nltk \n",
    "import spacy \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math \n",
    "from tqdm import tqdm \n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy \n",
    "\n",
    "# load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainingdata=\"\"\"get me documents with publish date greater than 10-Oct-2010\n",
    "get me companies with revenue greater than 500000\n",
    "companies with revenue above 510000\n",
    "companies with revenue in access of 500000\n",
    "companies with revenue exceeding 500000\n",
    "get me companies with revenue less than 500000\n",
    "get me companies with revenue lesser than $ 5b\n",
    "get me companies with revenue lesser than five billion\n",
    "what deals have revenue higher than $5b\n",
    "get me companies with revenue more than 500000\n",
    "Which are the companies having revenues of more than 1000000\n",
    "deals with deal size more than 200000\"\"\"\n",
    "trainingdata_semantics = \"\"\"ROOT, -, -, -, FIELD, FIELD, GTR, GTR, VAL\n",
    "ROOT, -, -, -, FIELD, GTR, GTR, VAL\n",
    "ROOT, -, FIELD, GTR, VAL\n",
    "ROOT, -, FIELD, GTR, GTR, GTR, VAL\n",
    "ROOT, -, FIELD, GTR, VAL\n",
    "ROOT, -, -, -, FIELD, LSR, LSR, VAL\n",
    "ROOT, -, -, -, FIELD, LSR, LSR, VAL, VAL\n",
    "ROOT, -, -, -, FIELD, LSR, LSR, VAL, VAL\n",
    "-, -, ROOT, FIELD, GTR, GTR, VAL, VAL\n",
    "ROOT, -, -, -, FIELD, GTR, GTR, VAL\n",
    "-, ROOT, -, -, -, FIELD, -, GTR, GTR, VAL\n",
    "ROOT, -, FIELD, FIELD, GTR, GTR, VAL\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "1\n",
      "{'heads': [0, 0, 0, 2, 5, 3, 5, 6, 7], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'FIELD', 'GTR', 'GTR', 'VAL']}\n",
      "***************************\n",
      "2\n",
      "{'heads': [0, 0, 0, 2, 3, 4, 5, 6], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'GTR', 'GTR', 'VAL']}\n",
      "***************************\n",
      "3\n",
      "{'heads': [0, 0, 1, 2, 3], 'deps': ['ROOT', '-', 'FIELD', 'GTR', 'VAL']}\n",
      "***************************\n",
      "4\n",
      "{'heads': [0, 0, 1, 2, 3, 4, 5], 'deps': ['ROOT', '-', 'FIELD', 'GTR', 'GTR', 'GTR', 'VAL']}\n",
      "***************************\n",
      "5\n",
      "{'heads': [0, 0, 1, 2, 3], 'deps': ['ROOT', '-', 'FIELD', 'GTR', 'VAL']}\n",
      "***************************\n",
      "6\n",
      "{'heads': [0, 0, 0, 2, 3, 7, 7, 0], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'LSR', 'LSR', 'VAL']}\n",
      "***************************\n",
      "7\n",
      "{'heads': [0, 0, 0, 2, 3, 4, 5, 8, 6], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'LSR', 'LSR', 'VAL', 'VAL']}\n",
      "***************************\n",
      "8\n",
      "{'heads': [0, 0, 0, 2, 3, 4, 5, 8, 6], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'LSR', 'LSR', 'VAL', 'VAL']}\n",
      "***************************\n",
      "9\n",
      "{'heads': [1, 2, 2, 2, 3, 4, 7, 5], 'deps': ['-', '-', 'ROOT', 'FIELD', 'GTR', 'GTR', 'VAL', 'VAL']}\n",
      "***************************\n",
      "10\n",
      "{'heads': [0, 0, 0, 2, 3, 7, 7, 0], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'GTR', 'GTR', 'VAL']}\n",
      "***************************\n",
      "11\n",
      "{'heads': [1, 1, 3, 1, 3, 4, 5, 9, 9, 6], 'deps': ['-', 'ROOT', '-', '-', '-', 'FIELD', '-', 'GTR', 'GTR', 'VAL']}\n",
      "***************************\n",
      "12\n",
      "{'heads': [0, 0, 3, 1, 6, 6, 0], 'deps': ['ROOT', '-', 'FIELD', 'FIELD', 'GTR', 'GTR', 'VAL']}\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[('get me documents with publish date greater than 10-Oct-2010', {'heads': [0, 0, 0, 2, 5, 3, 5, 6, 7], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'FIELD', 'GTR', 'GTR', 'VAL']}), ('get me companies with revenue greater than 500000', {'heads': [0, 0, 0, 2, 3, 4, 5, 6], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'GTR', 'GTR', 'VAL']}), ('companies with revenue above 510000', {'heads': [0, 0, 1, 2, 3], 'deps': ['ROOT', '-', 'FIELD', 'GTR', 'VAL']}), ('companies with revenue in access of 500000', {'heads': [0, 0, 1, 2, 3, 4, 5], 'deps': ['ROOT', '-', 'FIELD', 'GTR', 'GTR', 'GTR', 'VAL']}), ('companies with revenue exceeding 500000', {'heads': [0, 0, 1, 2, 3], 'deps': ['ROOT', '-', 'FIELD', 'GTR', 'VAL']}), ('get me companies with revenue less than 500000', {'heads': [0, 0, 0, 2, 3, 7, 7, 0], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'LSR', 'LSR', 'VAL']}), ('get me companies with revenue lesser than $ 5b', {'heads': [0, 0, 0, 2, 3, 4, 5, 8, 6], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'LSR', 'LSR', 'VAL', 'VAL']}), ('get me companies with revenue lesser than five billion', {'heads': [0, 0, 0, 2, 3, 4, 5, 8, 6], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'LSR', 'LSR', 'VAL', 'VAL']}), ('what deals have revenue higher than $5b', {'heads': [1, 2, 2, 2, 3, 4, 7, 5], 'deps': ['-', '-', 'ROOT', 'FIELD', 'GTR', 'GTR', 'VAL', 'VAL']}), ('get me companies with revenue more than 500000', {'heads': [0, 0, 0, 2, 3, 7, 7, 0], 'deps': ['ROOT', '-', '-', '-', 'FIELD', 'GTR', 'GTR', 'VAL']}), ('Which are the companies having revenues of more than 1000000', {'heads': [1, 1, 3, 1, 3, 4, 5, 9, 9, 6], 'deps': ['-', 'ROOT', '-', '-', '-', 'FIELD', '-', 'GTR', 'GTR', 'VAL']}), ('deals with deal size more than 200000', {'heads': [0, 0, 3, 1, 6, 6, 0], 'deps': ['ROOT', '-', 'FIELD', 'FIELD', 'GTR', 'GTR', 'VAL']})]\n"
     ]
    }
   ],
   "source": [
    "#Print heads\n",
    "lines = Trainingdata.split('\\n')\n",
    "sem_lines = trainingdata_semantics.split('\\n')\n",
    "\n",
    "training_data=[]\n",
    "sample_data=()\n",
    "k=1\n",
    "j=0\n",
    "for text in lines:\n",
    "    doc = nlp(text)\n",
    "    print(\"***************************\")\n",
    "    i = 0\n",
    "    head = {}\n",
    "    list = []\n",
    "    dep = []\n",
    "    toks = []\n",
    "    sem_deps = sem_lines[j].split(',')\n",
    "    j+=1\n",
    "    #i=1\n",
    "    for tok in doc:\n",
    "        #print(tok.head.idx)\n",
    "        list.append(tok.head.i)\n",
    "        #print(tok.dep_)\n",
    "        tokdep= str(tok.dep_) \n",
    "        #print(tokdep)\n",
    "        dep.append(sem_deps[i].strip())  ##tokdep)\n",
    "        i+=1\n",
    "        toks.append(tok.text)\n",
    "        #print(tok.text,\"-->\",tok.dep_,\"-->\",tok.pos_, tok.head, tok.lefts, tok.tag_)\n",
    "        #print([w for w in tok.lefts])\n",
    "    head[\"heads\"]=list\n",
    "    head[\"deps\"]=dep\n",
    "    #head[\"toks\"]=toks\n",
    "    print(k)\n",
    "    print(head)\n",
    "    k+=1\n",
    "    sample_data=(text, head)\n",
    "    #print(sample_data)\n",
    "    training_data.append(sample_data)\n",
    "print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "print(training_data)\n",
    "TRAIN_DATA=training_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-m None] [-o None] [-n 15]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\admin\\AppData\\Roaming\\jupyter\\runtime\\kernel-9f07568a-5f05-4073-8ed7-6198766a0efc.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(model=None, output_dir=None, n_iter=15):\n",
    "    \"\"\"Load the model, set up the pipeline and train the parser.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # We'll use the built-in dependency parser class, but we want to create a\n",
    "    # fresh instance â€“ just in case.\n",
    "    if \"parser\" in nlp.pipe_names:\n",
    "        nlp.remove_pipe(\"parser\")\n",
    "    parser = nlp.create_pipe(\"parser\")\n",
    "    nlp.add_pipe(parser, first=True)\n",
    "\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        for dep in annotations.get(\"deps\", []):\n",
    "            parser.add_label(dep)\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"parser\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train parser\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_model(nlp)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        test_model(nlp2)\n",
    "\n",
    "\n",
    "def test_model(nlp):\n",
    "    texts = [\n",
    "        \"deals with size more than 200100\",\n",
    "        \"get me companies with sales greater than 400000\",\n",
    "        \"companies with revenue in excess of 50000\",\n",
    "    ]\n",
    "    docs = nlp.pipe(texts)\n",
    "    for doc in docs:\n",
    "        print(doc.text)\n",
    "        print([(t.text, t.dep_, t.head.text) for t in doc if t.dep_ != \"-\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
